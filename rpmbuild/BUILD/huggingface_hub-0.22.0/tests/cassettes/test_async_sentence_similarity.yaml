interactions:
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      user-agent:
      - unknown/None; hf_hub/0.16.0.dev0; python/3.10.6; torch/1.12.1; tensorflow/2.11.0;
        fastcore/1.5.23
    method: GET
    uri: https://hub-ci.huggingface.co/api/tasks
  response:
    body:
      string: "{\"audio-classification\":{\"datasets\":[{\"description\":\"A benchmark
        of 10 different audio tasks.\",\"id\":\"superb\"}],\"demo\":{\"inputs\":[{\"filename\":\"audio.wav\",\"type\":\"audio\"}],\"outputs\":[{\"data\":[{\"label\":\"Up\",\"score\":0.2},{\"label\":\"Down\",\"score\":0.8}],\"type\":\"chart\"}]},\"metrics\":[{\"description\":\"\",\"id\":\"accuracy\"},{\"description\":\"\",\"id\":\"recall\"},{\"description\":\"\",\"id\":\"precision\"},{\"description\":\"\",\"id\":\"f1\"}],\"models\":[{\"description\":\"An
        easy-to-use model for Command Recognition.\",\"id\":\"speechbrain/google_speech_command_xvector\"},{\"description\":\"An
        Emotion Recognition model.\",\"id\":\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"},{\"description\":\"A
        language identification model.\",\"id\":\"facebook/mms-lid-126\"}],\"spaces\":[{\"description\":\"An
        application that can predict the language spoken in a given audio.\",\"id\":\"akhaliq/Speechbrain-audio-classification\"}],\"summary\":\"Audio
        classification is the task of assigning a label or class to a given audio.
        It can be used for recognizing which command a user is giving or the emotion
        of a statement, as well as identifying a speaker.\",\"widgetModels\":[\"facebook/mms-lid-126\"],\"youtubeId\":\"KWwzcmG98Ds\",\"id\":\"audio-classification\",\"label\":\"Audio
        Classification\",\"libraries\":[\"speechbrain\",\"transformers\"]},\"audio-to-audio\":{\"datasets\":[{\"description\":\"512-element
        X-vector embeddings of speakers from CMU ARCTIC dataset.\",\"id\":\"Matthijs/cmu-arctic-xvectors\"}],\"demo\":{\"inputs\":[{\"filename\":\"input.wav\",\"type\":\"audio\"}],\"outputs\":[{\"filename\":\"label-0.wav\",\"type\":\"audio\"},{\"filename\":\"label-1.wav\",\"type\":\"audio\"}]},\"metrics\":[{\"description\":\"The
        Signal-to-Noise ratio is the relationship between the target signal level
        and the background noise level. It is calculated as the logarithm of the target
        signal divided by the background noise, in decibels.\",\"id\":\"snri\"},{\"description\":\"The
        Signal-to-Distortion ratio is the relationship between the target signal and
        the sum of noise, interference, and artifact errors\",\"id\":\"sdri\"}],\"models\":[{\"description\":\"A
        solid model of audio source separation.\",\"id\":\"speechbrain/sepformer-wham\"},{\"description\":\"A
        speech enhancement model.\",\"id\":\"speechbrain/metricgan-plus-voicebank\"}],\"spaces\":[{\"description\":\"An
        application for speech separation.\",\"id\":\"younver/speechbrain-speech-separation\"},{\"description\":\"An
        application for audio style transfer.\",\"id\":\"nakas/audio-diffusion_style_transfer\"}],\"summary\":\"Audio-to-Audio
        is a family of tasks in which the input is an audio and the output is one
        or multiple generated audios. Some example tasks are speech enhancement and
        source separation.\",\"widgetModels\":[\"speechbrain/sepformer-wham\"],\"youtubeId\":\"iohj7nCCYoM\",\"id\":\"audio-to-audio\",\"label\":\"Audio-to-Audio\",\"libraries\":[\"asteroid\",\"speechbrain\"]},\"automatic-speech-recognition\":{\"datasets\":[{\"description\":\"An
        English dataset with 1,000 hours of data.\",\"id\":\"librispeech_asr\"},{\"description\":\"Dataset
        in 60 languages including demographic information.\",\"id\":\"common_voice\"},{\"description\":\"High
        quality, multi-speaker audio data and their transcriptions  in various languages.\",\"id\":\"openslr\"}],\"demo\":{\"inputs\":[{\"filename\":\"input.flac\",\"type\":\"audio\"}],\"outputs\":[{\"label\":\"Transcript\",\"content\":\"Going
        along slushy country roads and speaking to damp audiences in...\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"\",\"id\":\"wer\"},{\"description\":\"\",\"id\":\"cer\"}],\"models\":[{\"description\":\"A
        good generic ASR model.\",\"id\":\"facebook/wav2vec2-base-960h\"},{\"description\":\"A
        powerful ASR model.\",\"id\":\"openai/whisper-large-v2\"},{\"description\":\"An
        end-to-end model that performs Automatic Speech Recognition and Speech Translation.\",\"id\":\"facebook/s2t-small-mustc-en-fr-st\"}],\"spaces\":[{\"description\":\"A
        powerful general-purpose speech recognition application.\",\"id\":\"openai/whisper\"},{\"description\":\"Fastest
        speech recognition application.\",\"id\":\"sanchit-gandhi/whisper-jax\"},{\"description\":\"An
        application that transcribes speeches in YouTube videos.\",\"id\":\"jeffistyping/Youtube-Whisperer\"}],\"summary\":\"Automatic
        Speech Recognition (ASR), also known as Speech to Text (STT), is the task
        of transcribing a given audio to text. It has many applications, such as voice
        user interfaces.\",\"widgetModels\":[\"openai/whisper-large-v2\"],\"youtubeId\":\"TksaY_FDgnk\",\"id\":\"automatic-speech-recognition\",\"label\":\"Automatic
        Speech Recognition\",\"libraries\":[\"espnet\",\"nemo\",\"speechbrain\",\"transformers\",\"transformers.js\"]},\"conversational\":{\"datasets\":[{\"description\":\"A
        dataset of 7k conversations explicitly designed to exhibit multiple conversation
        modes: displaying personality, having empathy, and demonstrating knowledge.\",\"id\":\"blended_skill_talk\"},{\"description\":\"ConvAI
        is a dataset of human-to-bot conversations labeled for quality. This data
        can be used to train a metric for evaluating dialogue systems\",\"id\":\"conv_ai_2\"},{\"description\":\"EmpatheticDialogues,
        is a dataset of 25k conversations grounded in emotional situations\",\"id\":\"empathetic_dialogues\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"Hey
        my name is Julien! How are you?\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Answer\",\"content\":\"Hi
        Julien! My name is Julia! I am well.\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"BLEU
        score is calculated by counting the number of shared single or subsequent
        tokens between the generated sequence and the reference. Subsequent n tokens
        are called \u201Cn-grams\u201D. Unigram refers to a single token while bi-gram
        refers to token pairs and n-grams refer to n subsequent tokens. The score
        ranges from 0 to 1, where 1 means the translation perfectly matched and 0
        did not match at all\",\"id\":\"bleu\"}],\"models\":[{\"description\":\"A
        faster and smaller model than the famous BERT model.\",\"id\":\"facebook/blenderbot-400M-distill\"},{\"description\":\"DialoGPT
        is a large-scale pretrained dialogue response generation model for multiturn
        conversations.\",\"id\":\"microsoft/DialoGPT-large\"}],\"spaces\":[{\"description\":\"A
        chatbot based on Blender model.\",\"id\":\"EXFINITE/BlenderBot-UI\"}],\"summary\":\"Conversational
        response modelling is the task of generating conversational text that is relevant,
        coherent and knowledgable given a prompt. These models have applications in
        chatbots, and as a part of voice assistants\",\"widgetModels\":[\"facebook/blenderbot-400M-distill\"],\"youtubeId\":\"\",\"id\":\"conversational\",\"label\":\"Conversational\",\"libraries\":[\"transformers\"]},\"depth-estimation\":{\"datasets\":[{\"description\":\"NYU
        Depth V2 Dataset: Video dataset containing both RGB and depth sensor data\",\"id\":\"sayakpaul/nyu_depth_v2\"}],\"demo\":{\"inputs\":[{\"filename\":\"depth-estimation-input.jpg\",\"type\":\"img\"}],\"outputs\":[{\"filename\":\"depth-estimation-output.png\",\"type\":\"img\"}]},\"metrics\":[],\"models\":[{\"description\":\"Strong
        Depth Estimation model trained on 1.4 million images.\",\"id\":\"Intel/dpt-large\"},{\"description\":\"Strong
        Depth Estimation model trained on the KITTI dataset.\",\"id\":\"vinvino02/glpn-kitti\"}],\"spaces\":[{\"description\":\"An
        application that predicts the depth of an image and then reconstruct the 3D
        model as voxels.\",\"id\":\"radames/dpt-depth-estimation-3d-voxels\"},{\"description\":\"An
        application that can estimate the depth in a given image.\",\"id\":\"keras-io/Monocular-Depth-Estimation\"}],\"summary\":\"Depth
        estimation is the task of predicting depth of the objects present in an image.\",\"widgetModels\":[\"\"],\"youtubeId\":\"\",\"id\":\"depth-estimation\",\"label\":\"Depth
        Estimation\",\"libraries\":[\"transformers\"]},\"document-question-answering\":{\"datasets\":[{\"description\":\"Dataset
        from the 2020 DocVQA challenge. The documents are taken from the UCSF Industry
        Documents Library.\",\"id\":\"eliolio/docvqa\"}],\"demo\":{\"inputs\":[{\"label\":\"Question\",\"content\":\"What
        is the idea behind the consumer relations efficiency team?\",\"type\":\"text\"},{\"filename\":\"document-question-answering-input.png\",\"type\":\"img\"}],\"outputs\":[{\"label\":\"Answer\",\"content\":\"Balance
        cost efficiency with quality customer service\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"The
        evaluation metric for the DocVQA challenge is the Average Normalized Levenshtein
        Similarity (ANLS). This metric is flexible to character regognition errors
        and compares the predicted answer with the ground truth answer.\",\"id\":\"anls\"},{\"description\":\"Exact
        Match is a metric based on the strict character match of the predicted answer
        and the right answer. For answers predicted correctly, the Exact Match will
        be 1. Even if only one character is different, Exact Match will be 0\",\"id\":\"exact-match\"}],\"models\":[{\"description\":\"A
        LayoutLM model for the document QA task, fine-tuned on DocVQA and SQuAD2.0.\",\"id\":\"impira/layoutlm-document-qa\"},{\"description\":\"A
        special model for OCR-free Document QA task. Donut model fine-tuned on DocVQA.\",\"id\":\"naver-clova-ix/donut-base-finetuned-docvqa\"}],\"spaces\":[{\"description\":\"A
        robust document question answering application.\",\"id\":\"impira/docquery\"},{\"description\":\"An
        application that can answer questions from invoices.\",\"id\":\"impira/invoices\"}],\"summary\":\"Document
        Question Answering (also known as Document Visual Question Answering) is the
        task of answering questions on document images. Document question answering
        models take a (document, question) pair as input and return an answer in natural
        language. Models usually rely on multi-modal features, combining text, position
        of words (bounding-boxes) and image.\",\"widgetModels\":[\"impira/layoutlm-document-qa\"],\"youtubeId\":\"\",\"id\":\"document-question-answering\",\"label\":\"Document
        Question Answering\",\"libraries\":[\"transformers\"]},\"feature-extraction\":{\"datasets\":[{\"description\":\"Wikipedia
        dataset containing cleaned articles of all languages. Can be used to train
        `feature-extraction` models.\",\"id\":\"wikipedia\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"India,
        officially the Republic of India, is a country in South Asia.\",\"type\":\"text\"}],\"outputs\":[{\"table\":[[\"Dimension
        1\",\"Dimension 2\",\"Dimension 3\"],[\"2.583383083343506\",\"2.757075071334839\",\"0.9023529887199402\"],[\"8.29393482208252\",\"1.1071064472198486\",\"2.03399395942688\"],[\"-0.7754912972450256\",\"-1.647324562072754\",\"-0.6113331913948059\"],[\"0.07087723910808563\",\"1.5942802429199219\",\"1.4610432386398315\"]],\"type\":\"tabular\"}]},\"metrics\":[{\"description\":\"\",\"id\":\"\"}],\"models\":[{\"description\":\"A
        powerful feature extraction model for natural language processing tasks.\",\"id\":\"facebook/bart-base\"},{\"description\":\"A
        strong feature extraction model for coding tasks.\",\"id\":\"microsoft/codebert-base\"}],\"spaces\":[],\"summary\":\"Feature
        extraction refers to the process of transforming raw data into numerical features
        that can be processed while preserving the information in the original dataset.\",\"widgetModels\":[\"facebook/bart-base\"],\"id\":\"feature-extraction\",\"label\":\"Feature
        Extraction\",\"libraries\":[\"sentence-transformers\",\"transformers\",\"transformers.js\"]},\"fill-mask\":{\"datasets\":[{\"description\":\"A
        common dataset that is used to train models for many languages.\",\"id\":\"wikipedia\"},{\"description\":\"A
        large English dataset with text crawled from the web.\",\"id\":\"c4\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"The
        <mask> barked at me\",\"type\":\"text\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"wolf\",\"score\":0.487},{\"label\":\"dog\",\"score\":0.061},{\"label\":\"cat\",\"score\":0.058},{\"label\":\"fox\",\"score\":0.047},{\"label\":\"squirrel\",\"score\":0.025}]}]},\"metrics\":[{\"description\":\"Cross
        Entropy is a metric that calculates the difference between two probability
        distributions. Each probability distribution is the distribution of predicted
        words\",\"id\":\"cross_entropy\"},{\"description\":\"Perplexity is the exponential
        of the cross-entropy loss. It evaluates the probabilities assigned to the
        next word by the model. Lower perplexity indicates better performance\",\"id\":\"perplexity\"}],\"models\":[{\"description\":\"A
        faster and smaller model than the famous BERT model.\",\"id\":\"distilbert-base-uncased\"},{\"description\":\"A
        multilingual model trained on 100 languages.\",\"id\":\"xlm-roberta-base\"}],\"spaces\":[],\"summary\":\"Masked
        language modeling is the task of masking some of the words in a sentence and
        predicting which words should replace those masks. These models are useful
        when we want to get a statistical understanding of the language in which the
        model is trained in.\",\"widgetModels\":[\"distilroberta-base\"],\"youtubeId\":\"mqElG5QJWUg\",\"id\":\"fill-mask\",\"label\":\"Fill-Mask\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"image-classification\":{\"datasets\":[{\"description\":\"Benchmark
        dataset used for image classification with images that belong to 100 classes.\",\"id\":\"cifar100\"},{\"description\":\"Dataset
        consisting of images of garments.\",\"id\":\"fashion_mnist\"}],\"demo\":{\"inputs\":[{\"filename\":\"image-classification-input.jpeg\",\"type\":\"img\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"Egyptian
        cat\",\"score\":0.514},{\"label\":\"Tabby cat\",\"score\":0.193},{\"label\":\"Tiger
        cat\",\"score\":0.068}]}]},\"metrics\":[{\"description\":\"\",\"id\":\"accuracy\"},{\"description\":\"\",\"id\":\"recall\"},{\"description\":\"\",\"id\":\"precision\"},{\"description\":\"\",\"id\":\"f1\"}],\"models\":[{\"description\":\"A
        strong image classification model.\",\"id\":\"google/vit-base-patch16-224\"},{\"description\":\"A
        robust image classification model.\",\"id\":\"facebook/deit-base-distilled-patch16-224\"},{\"description\":\"A
        strong image classification model.\",\"id\":\"facebook/convnext-large-224\"}],\"spaces\":[{\"description\":\"An
        application that classifies what a given image is about.\",\"id\":\"nielsr/perceiver-image-classification\"}],\"summary\":\"Image
        classification is the task of assigning a label or class to an entire image.
        Images are expected to have only one class for each image. Image classification
        models take an image as input and return a prediction about which class the
        image belongs to.\",\"widgetModels\":[\"google/vit-base-patch16-224\"],\"youtubeId\":\"tjAIM7BOYhw\",\"id\":\"image-classification\",\"label\":\"Image
        Classification\",\"libraries\":[\"keras\",\"timm\",\"transformers\",\"transformers.js\"]},\"image-segmentation\":{\"datasets\":[{\"description\":\"Scene
        segmentation dataset.\",\"id\":\"scene_parse_150\"}],\"demo\":{\"inputs\":[{\"filename\":\"image-segmentation-input.jpeg\",\"type\":\"img\"}],\"outputs\":[{\"filename\":\"image-segmentation-output.png\",\"type\":\"img\"}]},\"metrics\":[{\"description\":\"Average
        Precision (AP) is the Area Under the PR Curve (AUC-PR). It is calculated for
        each semantic class separately\",\"id\":\"Average Precision\"},{\"description\":\"Mean
        Average Precision (mAP) is the overall average of the AP values\",\"id\":\"Mean
        Average Precision\"},{\"description\":\"Intersection over Union (IoU) is the
        overlap of segmentation masks. Mean IoU is the average of the IoU of all semantic
        classes\",\"id\":\"Mean Intersection over Union\"},{\"description\":\"AP\u03B1
        is the Average Precision at the IoU threshold of a \u03B1 value, for example,
        AP50 and AP75\",\"id\":\"AP\u03B1\"}],\"models\":[{\"description\":\"Solid
        panoptic segmentation model trained on the COCO 2017 benchmark dataset.\",\"id\":\"facebook/detr-resnet-50-panoptic\"},{\"description\":\"Semantic
        segmentation model trained on ADE20k benchmark dataset.\",\"id\":\"microsoft/beit-large-finetuned-ade-640-640\"},{\"description\":\"Semantic
        segmentation model trained on ADE20k benchmark dataset with 512x512 resolution.\",\"id\":\"nvidia/segformer-b0-finetuned-ade-512-512\"},{\"description\":\"Semantic
        segmentation model trained Cityscapes dataset.\",\"id\":\"facebook/mask2former-swin-large-cityscapes-semantic\"},{\"description\":\"Panoptic
        segmentation model trained COCO (common objects) dataset.\",\"id\":\"facebook/mask2former-swin-large-coco-panoptic\"}],\"spaces\":[{\"description\":\"A
        semantic segmentation application that can predict unseen instances out of
        the box.\",\"id\":\"facebook/ov-seg\"},{\"description\":\"One of the strongest
        segmentation applications.\",\"id\":\"jbrinkma/segment-anything\"},{\"description\":\"A
        semantic segmentation application that predicts human silhouettes.\",\"id\":\"keras-io/Human-Part-Segmentation\"},{\"description\":\"An
        instance segmentation application to predict neuronal cell types from microscopy
        images.\",\"id\":\"rashmi/sartorius-cell-instance-segmentation\"},{\"description\":\"An
        application that segments videos.\",\"id\":\"ArtGAN/Segment-Anything-Video\"},{\"description\":\"An
        panoptic segmentation application built for outdoor environments.\",\"id\":\"segments/panoptic-segment-anything\"}],\"summary\":\"Image
        Segmentation divides an image into segments where each pixel in the image
        is mapped to an object. This task has multiple variants such as instance segmentation,
        panoptic segmentation and semantic segmentation.\",\"widgetModels\":[\"facebook/detr-resnet-50-panoptic\"],\"youtubeId\":\"dKE8SIt9C-w\",\"id\":\"image-segmentation\",\"label\":\"Image
        Segmentation\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"image-to-image\":{\"datasets\":[{\"description\":\"Synthetic
        dataset, for image relighting\",\"id\":\"VIDIT\"},{\"description\":\"Multiple
        images of celebrities, used for facial expression translation\",\"id\":\"huggan/CelebA-faces\"}],\"demo\":{\"inputs\":[{\"filename\":\"image-to-image-input.jpeg\",\"type\":\"img\"}],\"outputs\":[{\"filename\":\"image-to-image-output.png\",\"type\":\"img\"}]},\"isPlaceholder\":false,\"metrics\":[{\"description\":\"Peak
        Signal to Noise Ratio (PSNR) is an approximation of the human perception,
        considering the ratio of the absolute intensity with respect to the variations.
        Measured in dB, a high value indicates a high fidelity.\",\"id\":\"PSNR\"},{\"description\":\"Structural
        Similarity Index (SSIM) is a perceptual metric which compares the luminance,
        contrast and structure of two images. The values of SSIM range between -1
        and 1, and higher values indicate closer resemblance to the original image.\",\"id\":\"SSIM\"},{\"description\":\"Inception
        Score (IS) is an analysis of the labels predicted by an image classification
        model when presented with a sample of the generated images.\",\"id\":\"IS\"}],\"models\":[{\"description\":\"A
        model that enhances images captured in low light conditions.\",\"id\":\"keras-io/low-light-image-enhancement\"},{\"description\":\"A
        model that increases the resolution of an image.\",\"id\":\"keras-io/super-resolution\"},{\"description\":\"A
        model that creates a set of variations of the input image in the style of
        DALL-E using Stable Diffusion.\",\"id\":\"lambdalabs/sd-image-variations-diffusers\"},{\"description\":\"A
        model that generates images based on segments in the input image and the text
        prompt.\",\"id\":\"mfidabel/controlnet-segment-anything\"},{\"description\":\"A
        model that takes an image and an instruction to edit the image.\",\"id\":\"timbrooks/instruct-pix2pix\"}],\"spaces\":[{\"description\":\"Image
        enhancer application for low light.\",\"id\":\"keras-io/low-light-image-enhancement\"},{\"description\":\"Style
        transfer application.\",\"id\":\"keras-io/neural-style-transfer\"},{\"description\":\"An
        application that generates images based on segment control.\",\"id\":\"mfidabel/controlnet-segment-anything\"},{\"description\":\"Image
        generation application that takes image control and text prompt.\",\"id\":\"hysts/ControlNet\"},{\"description\":\"Colorize
        any image using this app.\",\"id\":\"ioclab/brightness-controlnet\"},{\"description\":\"Edit
        images with instructions.\",\"id\":\"timbrooks/instruct-pix2pix\"}],\"summary\":\"Image-to-image
        is the task of transforming a source image to match the characteristics of
        a target image or a target image domain. Any image manipulation and enhancement
        is possible with image to image models.\",\"widgetModels\":[\"lllyasviel/sd-controlnet-canny\"],\"youtubeId\":\"\",\"id\":\"image-to-image\",\"label\":\"Image-to-Image\",\"libraries\":[]},\"image-to-text\":{\"datasets\":[{\"description\":\"Dataset
        from 12M image-text of Reddit\",\"id\":\"red_caps\"},{\"description\":\"Dataset
        from 3.3M images of Google\",\"id\":\"datasets/conceptual_captions\"}],\"demo\":{\"inputs\":[{\"filename\":\"savanna.jpg\",\"type\":\"img\"}],\"outputs\":[{\"label\":\"Detailed
        description\",\"content\":\"a herd of giraffes and zebras grazing in a field\",\"type\":\"text\"}]},\"metrics\":[],\"models\":[{\"description\":\"A
        robust image captioning model.\",\"id\":\"Salesforce/blip-image-captioning-large\"},{\"description\":\"A
        strong image captioning model.\",\"id\":\"nlpconnect/vit-gpt2-image-captioning\"},{\"description\":\"A
        strong optical character recognition model.\",\"id\":\"microsoft/trocr-base-printed\"},{\"description\":\"A
        strong visual question answering model for scientific diagrams.\",\"id\":\"google/pix2struct-ai2d-base\"},{\"description\":\"A
        strong captioning model for UI components.\",\"id\":\"google/pix2struct-widget-captioning-base\"},{\"description\":\"A
        captioning model for images that contain text.\",\"id\":\"google/pix2struct-textcaps-base\"}],\"spaces\":[{\"description\":\"A
        robust image captioning application.\",\"id\":\"flax-community/image-captioning\"},{\"description\":\"An
        application that transcribes handwritings into text.\",\"id\":\"nielsr/TrOCR-handwritten\"},{\"description\":\"An
        application that can caption images and answer questions about a given image.\",\"id\":\"Salesforce/BLIP\"},{\"description\":\"An
        application that can caption images and answer questions with a conversational
        agent.\",\"id\":\"Salesforce/BLIP2\"},{\"description\":\"An image captioning
        application that demonstrates the effect of noise on captions.\",\"id\":\"johko/capdec-image-captioning\"}],\"summary\":\"Image
        to text models output a text from a given image. Image captioning or optical
        character recognition can be considered as the most common applications of
        image to text.\",\"widgetModels\":[\"Salesforce/blip-image-captioning-base\"],\"youtubeId\":\"\",\"id\":\"image-to-text\",\"label\":\"Image-to-Text\",\"libraries\":[\"transformers.js\"]},\"object-detection\":{\"datasets\":[{\"description\":\"Widely
        used benchmark dataset for multiple Vision tasks.\",\"id\":\"merve/coco2017\"}],\"demo\":{\"inputs\":[{\"filename\":\"object-detection-input.jpg\",\"type\":\"img\"}],\"outputs\":[{\"filename\":\"object-detection-output.jpg\",\"type\":\"img\"}]},\"metrics\":[{\"description\":\"The
        Average Precision (AP) metric is the Area Under the PR Curve (AUC-PR). It
        is calculated for each class separately\",\"id\":\"Average Precision\"},{\"description\":\"The
        Mean Average Precision (mAP) metric is the overall average of the AP values\",\"id\":\"Mean
        Average Precision\"},{\"description\":\"The AP\u03B1 metric is the Average
        Precision at the IoU threshold of a \u03B1 value, for example, AP50 and AP75\",\"id\":\"AP\u03B1\"}],\"models\":[{\"description\":\"Solid
        object detection model trained on the benchmark dataset COCO 2017.\",\"id\":\"facebook/detr-resnet-50\"},{\"description\":\"Strong
        object detection model trained on ImageNet-21k dataset.\",\"id\":\"microsoft/beit-base-patch16-224-pt22k-ft22k\"}],\"spaces\":[{\"description\":\"An
        object detection application that can detect unseen objects out of the box.\",\"id\":\"adirik/OWL-ViT\"},{\"description\":\"An
        application that contains various object detection models to try from.\",\"id\":\"Gradio-Blocks/Object-Detection-With-DETR-and-YOLOS\"},{\"description\":\"An
        application that shows multiple cutting edge techniques for object detection
        and tracking.\",\"id\":\"kadirnar/torchyolo\"},{\"description\":\"An object
        tracking, segmentation and inpainting application.\",\"id\":\"VIPLab/Track-Anything\"}],\"summary\":\"Object
        Detection models allow users to identify objects of certain defined classes.
        Object detection models receive an image as input and output the images with
        bounding boxes and labels on detected objects.\",\"widgetModels\":[\"facebook/detr-resnet-50\"],\"youtubeId\":\"WdAeKSOpxhw\",\"id\":\"object-detection\",\"label\":\"Object
        Detection\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"video-classification\":{\"datasets\":[{\"description\":\"Benchmark
        dataset used for video classification with videos that belong to 400 classes.\",\"id\":\"kinetics400\"}],\"demo\":{\"inputs\":[{\"filename\":\"video-classification-input.gif\",\"type\":\"img\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"Playing
        Guitar\",\"score\":0.514},{\"label\":\"Playing Tennis\",\"score\":0.193},{\"label\":\"Cooking\",\"score\":0.068}]}]},\"metrics\":[{\"description\":\"\",\"id\":\"accuracy\"},{\"description\":\"\",\"id\":\"recall\"},{\"description\":\"\",\"id\":\"precision\"},{\"description\":\"\",\"id\":\"f1\"}],\"models\":[{\"description\":\"Strong
        Video Classification model trained on the Kinects 400 dataset.\",\"id\":\"MCG-NJU/videomae-base-finetuned-kinetics\"},{\"description\":\"Strong
        Video Classification model trained on the Kinects 400 dataset.\",\"id\":\"microsoft/xclip-base-patch32\"}],\"spaces\":[{\"description\":\"An
        application that classifies video at different timestamps.\",\"id\":\"nateraw/lavila\"},{\"description\":\"An
        application that classifies video.\",\"id\":\"fcakyon/video-classification\"}],\"summary\":\"Video
        classification is the task of assigning a label or class to an entire video.
        Videos are expected to have only one class for each video. Video classification
        models take a video as input and return a prediction about which class the
        video belongs to.\",\"widgetModels\":[],\"youtubeId\":\"\",\"id\":\"video-classification\",\"label\":\"Video
        Classification\",\"libraries\":[]},\"question-answering\":{\"datasets\":[{\"description\":\"A
        famous question answering dataset based on English articles from Wikipedia.\",\"id\":\"squad_v2\"},{\"description\":\"A
        dataset of aggregated anonymized actual queries issued to the Google search
        engine.\",\"id\":\"natural_questions\"}],\"demo\":{\"inputs\":[{\"label\":\"Question\",\"content\":\"Which
        name is also used to describe the Amazon rainforest in English?\",\"type\":\"text\"},{\"label\":\"Context\",\"content\":\"The
        Amazon rainforest, also known in English as Amazonia or the Amazon Jungle\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Answer\",\"content\":\"Amazonia\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"Exact
        Match is a metric based on the strict character match of the predicted answer
        and the right answer. For answers predicted correctly, the Exact Match will
        be 1. Even if only one character is different, Exact Match will be 0\",\"id\":\"exact-match\"},{\"description\":\"
        The F1-Score metric is useful if we value both false positives and false negatives
        equally. The F1-Score is calculated on each word in the predicted sequence
        against the correct answer\",\"id\":\"f1\"}],\"models\":[{\"description\":\"A
        robust baseline model for most question answering domains.\",\"id\":\"deepset/roberta-base-squad2\"},{\"description\":\"A
        special model that can answer questions from tables!\",\"id\":\"google/tapas-base-finetuned-wtq\"}],\"spaces\":[{\"description\":\"An
        application that can answer a long question from Wikipedia.\",\"id\":\"deepset/wikipedia-assistant\"}],\"summary\":\"Question
        Answering models can retrieve the answer to a question from a given text,
        which is useful for searching for an answer in a document. Some question answering
        models can generate answers without context!\",\"widgetModels\":[\"deepset/roberta-base-squad2\"],\"youtubeId\":\"ajPx5LwJD-I\",\"id\":\"question-answering\",\"label\":\"Question
        Answering\",\"libraries\":[\"adapter-transformers\",\"allennlp\",\"transformers\",\"transformers.js\"]},\"reinforcement-learning\":{\"datasets\":[{\"description\":\"A
        curation of widely used datasets for Data Driven Deep Reinforcement Learning
        (D4RL)\",\"id\":\"edbeeching/decision_transformer_gym_replay\"}],\"demo\":{\"inputs\":[{\"label\":\"State\",\"content\":\"Red
        traffic light, pedestrians are about to pass.\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Action\",\"content\":\"Stop
        the car.\",\"type\":\"text\"},{\"label\":\"Next State\",\"content\":\"Yellow
        light, pedestrians have crossed.\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"Accumulated
        reward across all time steps discounted by a factor that ranges between 0
        and 1 and determines how much the agent optimizes for future relative to immediate
        rewards. Measures how good is the policy ultimately found by a given algorithm
        considering uncertainty over the future.\",\"id\":\"Discounted Total Reward\"},{\"description\":\"Average
        return obtained after running the policy for a certain number of evaluation
        episodes. As opposed to total reward, mean reward considers how much reward
        a given algorithm receives while learning.\",\"id\":\"Mean Reward\"},{\"description\":\"Measures
        how good a given algorithm is after a predefined time. Some algorithms may
        be guaranteed to converge to optimal behavior across many time steps. However,
        an agent that reaches an acceptable level of optimality after a given time
        horizon may be preferable to one that ultimately reaches optimality but takes
        a long time.\",\"id\":\"Level of Performance After Some Time\"}],\"models\":[{\"description\":\"A
        Reinforcement Learning model trained on expert data from the Gym Hopper environment\",\"id\":\"edbeeching/decision-transformer-gym-hopper-expert\"},{\"description\":\"A
        PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and
        the RL Zoo.\",\"id\":\"HumanCompatibleAI/ppo-seals-CartPole-v0\"}],\"spaces\":[{\"description\":\"An
        application for a cute puppy agent learning to catch a stick.\",\"id\":\"ThomasSimonini/Huggy\"},{\"description\":\"An
        application to play Snowball Fight with a reinforcement learning agent.\",\"id\":\"ThomasSimonini/SnowballFight\"}],\"summary\":\"Reinforcement
        learning is the computational approach of learning from action by interacting
        with an environment through trial and error and receiving rewards (negative
        or positive) as feedback\",\"widgetModels\":[],\"youtubeId\":\"q0BiUn5LiBc\",\"id\":\"reinforcement-learning\",\"label\":\"Reinforcement
        Learning\",\"libraries\":[\"transformers\",\"stable-baselines3\",\"ml-agents\",\"sample-factory\"]},\"sentence-similarity\":{\"datasets\":[{\"description\":\"Bing
        queries with relevant passages from various web sources.\",\"id\":\"ms_marco\"}],\"demo\":{\"inputs\":[{\"label\":\"Source
        sentence\",\"content\":\"Machine learning is so easy.\",\"type\":\"text\"},{\"label\":\"Sentences
        to compare to\",\"content\":\"Deep learning is so straightforward.\",\"type\":\"text\"},{\"label\":\"\",\"content\":\"This
        is so difficult, like rocket science.\",\"type\":\"text\"},{\"label\":\"\",\"content\":\"I
        can't believe how much I struggled with this.\",\"type\":\"text\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"Deep
        learning is so straightforward.\",\"score\":0.623},{\"label\":\"This is so
        difficult, like rocket science.\",\"score\":0.413},{\"label\":\"I can't believe
        how much I struggled with this.\",\"score\":0.256}]}]},\"metrics\":[{\"description\":\"Reciprocal
        Rank is a measure used to rank the relevancy of documents given a set of documents.
        Reciprocal Rank is the reciprocal of the rank of the document retrieved, meaning,
        if the rank is 3, the Reciprocal Rank is 0.33. If the rank is 1, the Reciprocal
        Rank is 1\",\"id\":\"Mean Reciprocal Rank\"},{\"description\":\"The similarity
        of the embeddings is evaluated mainly on cosine similarity. It is calculated
        as the cosine of the angle between two vectors. It is particularly useful
        when your texts are not the same length\",\"id\":\"Cosine Similarity\"}],\"models\":[{\"description\":\"This
        model works well for sentences and paragraphs and can be used for clustering/grouping
        and semantic searches.\",\"id\":\"sentence-transformers/all-mpnet-base-v2\"},{\"description\":\"A
        multilingual model trained for FAQ retrieval.\",\"id\":\"clips/mfaq\"}],\"spaces\":[{\"description\":\"An
        application that leverages sentence similarity to answer questions from YouTube
        videos.\",\"id\":\"Gradio-Blocks/Ask_Questions_To_YouTube_Videos\"},{\"description\":\"An
        application that retrieves relevant PubMed abstracts for a given online article
        which can be used as further references.\",\"id\":\"Gradio-Blocks/pubmed-abstract-retriever\"},{\"description\":\"An
        application that leverages sentence similarity to summarize text.\",\"id\":\"nickmuchi/article-text-summarizer\"},{\"description\":\"A
        guide that explains how Sentence Transformers can be used for semantic search.\",\"id\":\"sentence-transformers/Sentence_Transformers_for_semantic_search\"}],\"summary\":\"Sentence
        Similarity is the task of determining how similar two texts are. Sentence
        similarity models convert input texts into vectors (embeddings) that capture
        semantic information and calculate how close (similar) they are between them.
        This task is particularly useful for information retrieval and clustering/grouping.\",\"widgetModels\":[\"sentence-transformers/all-MiniLM-L6-v2\"],\"youtubeId\":\"VCZq5AkbNEU\",\"id\":\"sentence-similarity\",\"label\":\"Sentence
        Similarity\",\"libraries\":[\"sentence-transformers\",\"spacy\",\"transformers.js\"]},\"summarization\":{\"datasets\":[{\"description\":\"News
        articles in five different languages along with their summaries. Widely used
        for benchmarking multilingual summarization models.\",\"id\":\"mlsum\"},{\"description\":\"English
        conversations and their summaries. Useful for benchmarking conversational
        agents.\",\"id\":\"samsum\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"The
        tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey
        building, and the tallest structure in Paris. Its base is square, measuring
        125 metres (410 ft) on each side. It was the first structure to reach a height
        of 300 metres. Excluding transmitters, the Eiffel Tower is the second tallest
        free-standing structure in France after the Millau Viaduct.\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Output\",\"content\":\"The
        tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey
        building. It was the first structure to reach a height of 300 metres.\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"The
        generated sequence is compared against its summary, and the overlap of tokens
        are counted. ROUGE-N refers to overlap of N subsequent tokens, ROUGE-1 refers
        to overlap of single tokens and ROUGE-2 is the overlap of two subsequent tokens.\",\"id\":\"rouge\"}],\"models\":[{\"description\":\"A
        strong summarization model trained on English news articles. Excels at generating
        factual summaries.\",\"id\":\"facebook/bart-large-cnn\"},{\"description\":\"A
        summarization model trained on medical articles.\",\"id\":\"google/bigbird-pegasus-large-pubmed\"}],\"spaces\":[{\"description\":\"An
        application that can summarize long paragraphs.\",\"id\":\"pszemraj/summarize-long-text\"},{\"description\":\"A
        much needed summarization application for terms and conditions.\",\"id\":\"ml6team/distilbart-tos-summarizer-tosdr\"},{\"description\":\"An
        application that summarizes long documents.\",\"id\":\"pszemraj/document-summarization\"},{\"description\":\"An
        application that can detect errors in abstractive summarization.\",\"id\":\"ml6team/post-processing-summarization\"}],\"summary\":\"Summarization
        is the task of producing a shorter version of a document while preserving
        its important information. Some models can extract text from the original
        input, while other models can generate entirely new text.\",\"widgetModels\":[\"sshleifer/distilbart-cnn-12-6\"],\"youtubeId\":\"yHnr5Dk2zCI\",\"id\":\"summarization\",\"label\":\"Summarization\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"table-question-answering\":{\"datasets\":[{\"description\":\"The
        WikiTableQuestions dataset is a large-scale dataset for the task of question
        answering on semi-structured tables.\",\"id\":\"wikitablequestions\"},{\"description\":\"WikiSQL
        is a dataset of 80654 hand-annotated examples of questions and SQL queries
        distributed across 24241 tables from Wikipedia.\",\"id\":\"wikisql\"}],\"demo\":{\"inputs\":[{\"table\":[[\"Rank\",\"Name\",\"No.of
        reigns\",\"Combined days\"],[\"1\",\"lou Thesz\",\"3\",\"3749\"],[\"2\",\"Ric
        Flair\",\"8\",\"3103\"],[\"3\",\"Harley Race\",\"7\",\"1799\"]],\"type\":\"tabular\"},{\"label\":\"Question\",\"content\":\"What
        is the number of reigns for Harley Race?\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Result\",\"content\":\"7\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"Checks
        whether the predicted answer(s) is the same as the ground-truth answer(s).\",\"id\":\"Denotation
        Accuracy\"}],\"models\":[{\"description\":\"A table question answering model
        that is capable of neural SQL execution, i.e., employ TAPEX to execute a SQL
        query on a given table.\",\"id\":\"microsoft/tapex-base\"},{\"description\":\"A
        robust table question answering model.\",\"id\":\"google/tapas-base-finetuned-wtq\"}],\"spaces\":[{\"description\":\"An
        application that answers questions based on table CSV files.\",\"id\":\"katanaml/table-query\"}],\"summary\":\"Table
        Question Answering (Table QA) is the answering a question about an information
        on a given table.\",\"widgetModels\":[\"google/tapas-base-finetuned-wtq\"],\"id\":\"table-question-answering\",\"label\":\"Table
        Question Answering\",\"libraries\":[\"transformers\"]},\"tabular-classification\":{\"datasets\":[{\"description\":\"A
        comprehensive curation of datasets covering all benchmarks.\",\"id\":\"inria-soda/tabular-benchmark\"}],\"demo\":{\"inputs\":[{\"table\":[[\"Glucose\",\"Blood
        Pressure \",\"Skin Thickness\",\"Insulin\",\"BMI\"],[\"148\",\"72\",\"35\",\"0\",\"33.6\"],[\"150\",\"50\",\"30\",\"0\",\"35.1\"],[\"141\",\"60\",\"29\",\"1\",\"39.2\"]],\"type\":\"tabular\"}],\"outputs\":[{\"table\":[[\"Diabetes\"],[\"1\"],[\"1\"],[\"0\"]],\"type\":\"tabular\"}]},\"metrics\":[{\"description\":\"\",\"id\":\"accuracy\"},{\"description\":\"\",\"id\":\"recall\"},{\"description\":\"\",\"id\":\"precision\"},{\"description\":\"\",\"id\":\"f1\"}],\"models\":[{\"description\":\"Breast
        cancer prediction model based on decision trees.\",\"id\":\"scikit-learn/cancer-prediction-trees\"}],\"spaces\":[{\"description\":\"An
        application that can predict defective products on a production line.\",\"id\":\"scikit-learn/tabular-playground\"},{\"description\":\"An
        application that compares various tabular classification techniques on different
        datasets.\",\"id\":\"scikit-learn/classification\"}],\"summary\":\"Tabular
        classification is the task of classifying a target category (a group) based
        on set of attributes.\",\"widgetModels\":[\"scikit-learn/tabular-playground\"],\"youtubeId\":\"\",\"id\":\"tabular-classification\",\"label\":\"Tabular
        Classification\",\"libraries\":[\"sklearn\"]},\"tabular-regression\":{\"datasets\":[{\"description\":\"A
        comprehensive curation of datasets covering all benchmarks.\",\"id\":\"inria-soda/tabular-benchmark\"}],\"demo\":{\"inputs\":[{\"table\":[[\"Car
        Name\",\"Horsepower\",\"Weight\"],[\"ford torino\",\"140\",\"3,449\"],[\"amc
        hornet\",\"97\",\"2,774\"],[\"toyota corolla\",\"65\",\"1,773\"]],\"type\":\"tabular\"}],\"outputs\":[{\"table\":[[\"MPG
        (miles per gallon)\"],[\"17\"],[\"18\"],[\"31\"]],\"type\":\"tabular\"}]},\"metrics\":[{\"description\":\"\",\"id\":\"mse\"},{\"description\":\"Coefficient
        of determination (or R-squared) is a measure of how well the model fits the
        data. Higher R-squared is considered a better fit.\",\"id\":\"r-squared\"}],\"models\":[{\"description\":\"Fish
        weight prediction based on length measurements and species.\",\"id\":\"scikit-learn/Fish-Weight\"}],\"spaces\":[{\"description\":\"An
        application that can predict weight of a fish based on set of attributes.\",\"id\":\"scikit-learn/fish-weight-prediction\"}],\"summary\":\"Tabular
        regression is the task of predicting a numerical value given a set of attributes.\",\"widgetModels\":[\"scikit-learn/Fish-Weight\"],\"youtubeId\":\"\",\"id\":\"tabular-regression\",\"label\":\"Tabular
        Regression\",\"libraries\":[\"sklearn\"]},\"text-classification\":{\"datasets\":[{\"description\":\"A
        widely used dataset used to benchmark multiple variants of text classification.\",\"id\":\"glue\"},{\"description\":\"A
        text classification dataset used to benchmark natural language inference models\",\"id\":\"snli\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"I
        love Hugging Face!\",\"type\":\"text\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"POSITIVE\",\"score\":0.9},{\"label\":\"NEUTRAL\",\"score\":0.1},{\"label\":\"NEGATIVE\",\"score\":0}]}]},\"metrics\":[{\"description\":\"\",\"id\":\"accuracy\"},{\"description\":\"\",\"id\":\"recall\"},{\"description\":\"\",\"id\":\"precision\"},{\"description\":\"The
        F1 metric is the harmonic mean of the precision and recall. It can be calculated
        as: F1 = 2 * (precision * recall) / (precision + recall)\",\"id\":\"f1\"}],\"models\":[{\"description\":\"A
        robust model trained for sentiment analysis.\",\"id\":\"distilbert-base-uncased-finetuned-sst-2-english\"},{\"description\":\"Multi-genre
        natural language inference model.\",\"id\":\"roberta-large-mnli\"}],\"spaces\":[{\"description\":\"An
        application that can classify financial sentiment.\",\"id\":\"IoannisTr/Tech_Stocks_Trading_Assistant\"},{\"description\":\"A
        dashboard that contains various text classification tasks.\",\"id\":\"miesnerjacob/Multi-task-NLP\"},{\"description\":\"An
        application that analyzes user reviews in healthcare.\",\"id\":\"spacy/healthsea-demo\"}],\"summary\":\"Text
        Classification is the task of assigning a label or class to a given text.
        Some use cases are sentiment analysis, natural language inference, and assessing
        grammatical correctness.\",\"widgetModels\":[\"distilbert-base-uncased-finetuned-sst-2-english\"],\"youtubeId\":\"leNG9fN9FQU\",\"id\":\"text-classification\",\"label\":\"Text
        Classification\",\"libraries\":[\"adapter-transformers\",\"spacy\",\"transformers\",\"transformers.js\"]},\"text-generation\":{\"datasets\":[{\"description\":\"A
        large multilingual dataset of text crawled from the web.\",\"id\":\"mc4\"},{\"description\":\"Diverse
        open-source data consisting of 22 smaller high-quality datasets. It was used
        to train GPT-Neo.\",\"id\":\"the_pile\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"Once
        upon a time,\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Output\",\"content\":\"Once
        upon a time, we knew that our ancestors were on the verge of extinction. The
        great explorers and poets of the Old World, from Alexander the Great to Chaucer,
        are dead and gone. A good many of our ancient explorers and poets have\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"Cross
        Entropy is a metric that calculates the difference between two probability
        distributions. Each probability distribution is the distribution of predicted
        words\",\"id\":\"Cross Entropy\"},{\"description\":\"The Perplexity metric
        is the exponential of the cross-entropy loss. It evaluates the probabilities
        assigned to the next word by the model. Lower perplexity indicates better
        performance\",\"id\":\"Perplexity\"}],\"models\":[{\"description\":\"A large
        language model trained for text generation.\",\"id\":\"bigscience/bloom-560m\"},{\"description\":\"A
        large code generation model that can generate code in 80+ languages.\",\"id\":\"bigcode/starcoder\"},{\"description\":\"A
        model trained to follow instructions, uses Pythia-12b as base model.\",\"id\":\"databricks/dolly-v2-12b\"},{\"description\":\"A
        model trained to follow instructions curated by community, uses Pythia-12b
        as base model.\",\"id\":\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"},{\"description\":\"A
        large language model trained to generate text in English.\",\"id\":\"stabilityai/stablelm-tuned-alpha-7b\"},{\"description\":\"A
        model trained to follow instructions, based on mosaicml/mpt-7b.\",\"id\":\"mosaicml/mpt-7b-instruct\"},{\"description\":\"A
        large language model trained to generate text in English.\",\"id\":\"EleutherAI/pythia-12b\"},{\"description\":\"A
        large text-to-text model trained to follow instructions.\",\"id\":\"google/flan-ul2\"},{\"description\":\"A
        large and powerful text generation model.\",\"id\":\"tiiuae/falcon-40b\"}],\"spaces\":[{\"description\":\"A
        robust text generation model that can perform various tasks through natural
        language prompting.\",\"id\":\"huggingface/bloom_demo\"},{\"description\":\"An
        text generation based application that can write code for 80+ languages.\",\"id\":\"bigcode/bigcode-playground\"},{\"description\":\"An
        text generation based application for conversations.\",\"id\":\"h2oai/h2ogpt-chatbot\"},{\"description\":\"An
        text generation application that combines OpenAI and Hugging Face models.\",\"id\":\"microsoft/HuggingGPT\"},{\"description\":\"An
        text generation application that uses StableLM-tuned-alpha-7b.\",\"id\":\"stabilityai/stablelm-tuned-alpha-chat\"},{\"description\":\"An
        UI that uses StableLM-tuned-alpha-7b.\",\"id\":\"togethercomputer/OpenChatKit\"}],\"summary\":\"Generating
        text is the task of producing new text. These models can, for example, fill
        in incomplete text or paraphrase.\",\"widgetModels\":[\"tiiuae/falcon-7b-instruct\"],\"youtubeId\":\"Vpjb1lu0MDk\",\"id\":\"text-generation\",\"label\":\"Text
        Generation\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"text-to-image\":{\"datasets\":[{\"description\":\"RedCaps
        is a large-scale dataset of 12M image-text pairs collected from Reddit.\",\"id\":\"red_caps\"},{\"description\":\"Conceptual
        Captions is a dataset consisting of ~3.3M images annotated with captions.\",\"id\":\"conceptual_captions\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"A
        city above clouds, pastel colors, Victorian style\",\"type\":\"text\"}],\"outputs\":[{\"filename\":\"image.jpeg\",\"type\":\"img\"}]},\"metrics\":[],\"models\":[{\"description\":\"A
        latent text-to-image diffusion model capable of generating photo-realistic
        images given any text input.\",\"id\":\"CompVis/stable-diffusion-v1-4\"},{\"description\":\"A
        model that can be used to generate images based on text prompts. The DALL\xB7E
        Mega model is the largest version of DALLE Mini.\",\"id\":\"dalle-mini/dalle-mega\"},{\"description\":\"A
        text-to-image model that can generate coherent text inside image.\",\"id\":\"DeepFloyd/IF-I-XL-v1.0\"},{\"description\":\"A
        powerful text-to-image model.\",\"id\":\"kakaobrain/karlo-v1-alpha\"}],\"spaces\":[{\"description\":\"A
        powerful text-to-image application.\",\"id\":\"stabilityai/stable-diffusion\"},{\"description\":\"An
        text-to-image application that can generate coherent text inside the image.\",\"id\":\"DeepFloyd/IF\"},{\"description\":\"An
        powerful text-to-image application that can generate images.\",\"id\":\"kakaobrain/karlo\"},{\"description\":\"An
        powerful text-to-image application that can generates 3D representations.\",\"id\":\"hysts/Shap-E\"}],\"summary\":\"Generates
        images from input text. These models can be used to generate and modify images
        based on text prompts.\",\"widgetModels\":[\"CompVis/stable-diffusion-v1-4\"],\"youtubeId\":\"\",\"id\":\"text-to-image\",\"label\":\"Text-to-Image\",\"libraries\":[]},\"text-to-speech\":{\"datasets\":[{\"description\":\"Thousands
        of short audio clips of a single speaker.\",\"id\":\"lj_speech\"},{\"description\":\"Multi-speaker
        English dataset.\",\"id\":\"LibriTTS\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"I
        love audio models on the Hub!\",\"type\":\"text\"}],\"outputs\":[{\"filename\":\"audio.wav\",\"type\":\"audio\"}]},\"metrics\":[{\"description\":\"The
        Mel Cepstral Distortion (MCD) metric is used to calculate the quality of generated
        speech.\",\"id\":\"mel cepstral distortion\"}],\"models\":[{\"description\":\"An
        end-to-end speech synthesis model.\",\"id\":\"microsoft/speecht5_tts\"},{\"description\":\"An
        multi-lingual TTS model.\",\"id\":\"facebook/mms-tts\"},{\"description\":\"A
        powerful TTS model.\",\"id\":\"suno/bark\"}],\"spaces\":[{\"description\":\"An
        application for end-to-end text-to-speech.\",\"id\":\"rendchevi/nix-tts\"},{\"description\":\"An
        application that contains multiple speech recognition models for various languages
        and datasets.\",\"id\":\"coqui/CoquiTTS\"},{\"description\":\"An application
        that synthesizes speech for various speaker types.\",\"id\":\"Matthijs/speecht5-tts-demo\"}],\"summary\":\"Text-to-Speech
        (TTS) is the task of generating natural sounding speech given text input.
        TTS models can be extended to have a single model that generates speech for
        multiple speakers and multiple languages.\",\"widgetModels\":[\"microsoft/speecht5_tts\"],\"youtubeId\":\"NW62DpzJ274\",\"id\":\"text-to-speech\",\"label\":\"Text-to-Speech\",\"libraries\":[\"espnet\",\"tensorflowtts\"]},\"text-to-video\":{\"datasets\":[],\"demo\":{\"inputs\":[],\"outputs\":[]},\"isPlaceholder\":true,\"metrics\":[],\"models\":[],\"spaces\":[],\"summary\":\"\",\"widgetModels\":[],\"id\":\"text-to-video\",\"label\":\"Text-to-Video\",\"libraries\":[]},\"token-classification\":{\"datasets\":[{\"description\":\"A
        widely used dataset useful to benchmark named entity recognition models.\",\"id\":\"conll2003\"},{\"description\":\"A
        multilingual dataset of Wikipedia articles annotated for named entity recognition
        in over 150 different languages.\",\"id\":\"wikiann\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"My
        name is Omar and I live in Z\xFCrich.\",\"type\":\"text\"}],\"outputs\":[{\"text\":\"My
        name is Omar and I live in Z\xFCrich.\",\"tokens\":[{\"type\":\"PERSON\",\"start\":11,\"end\":15},{\"type\":\"GPE\",\"start\":30,\"end\":36}],\"type\":\"text-with-tokens\"}]},\"metrics\":[{\"description\":\"\",\"id\":\"accuracy\"},{\"description\":\"\",\"id\":\"recall\"},{\"description\":\"\",\"id\":\"precision\"},{\"description\":\"\",\"id\":\"f1\"}],\"models\":[{\"description\":\"A
        robust performance model to identify people, locations, organizations and
        names of miscellaneous entities.\",\"id\":\"dslim/bert-base-NER\"},{\"description\":\"Flair
        models are typically the state of the art in named entity recognition tasks.\",\"id\":\"flair/ner-english\"}],\"spaces\":[{\"description\":\"An
        application that can recognizes entities, extracts noun chunks and recognizes
        various linguistic features of each token.\",\"id\":\"spacy/gradio_pipeline_visualizer\"}],\"summary\":\"Token
        classification is a natural language understanding task in which a label is
        assigned to some tokens in a text. Some popular token classification subtasks
        are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models
        could be trained to identify specific entities in a text, such as dates, individuals
        and places; and PoS tagging would identify, for example, which words in a
        text are verbs, nouns, and punctuation marks.\",\"widgetModels\":[\"dslim/bert-base-NER\"],\"youtubeId\":\"wVHdVlPScxA\",\"id\":\"token-classification\",\"label\":\"Token
        Classification\",\"libraries\":[\"adapter-transformers\",\"flair\",\"spacy\",\"span-marker\",\"stanza\",\"transformers\",\"transformers.js\"]},\"translation\":{\"datasets\":[{\"description\":\"A
        dataset of copyright-free books translated into 16 different languages.\",\"id\":\"opus_books\"},{\"description\":\"An
        example of translation between programming languages. This dataset consists
        of functions in Java and C#.\",\"id\":\"code_x_glue_cc_code_to_code_trans\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"My
        name is Omar and I live in Z\xFCrich.\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Output\",\"content\":\"Mein
        Name ist Omar und ich wohne in Z\xFCrich.\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"BLEU
        score is calculated by counting the number of shared single or subsequent
        tokens between the generated sequence and the reference. Subsequent n tokens
        are called \u201Cn-grams\u201D. Unigram refers to a single token while bi-gram
        refers to token pairs and n-grams refer to n subsequent tokens. The score
        ranges from 0 to 1, where 1 means the translation perfectly matched and 0
        did not match at all\",\"id\":\"bleu\"},{\"description\":\"\",\"id\":\"sacrebleu\"}],\"models\":[{\"description\":\"A
        model that translates from English to French.\",\"id\":\"Helsinki-NLP/opus-mt-en-fr\"},{\"description\":\"A
        general-purpose Transformer that can be used to translate from English to
        German, French, or Romanian.\",\"id\":\"t5-base\"}],\"spaces\":[{\"description\":\"An
        application that can translate between 100 languages.\",\"id\":\"Iker/Translate-100-languages\"},{\"description\":\"An
        application that can translate between English, Spanish and Hindi.\",\"id\":\"EuroPython2022/Translate-with-Bloom\"}],\"summary\":\"Translation
        is the task of converting text from one language to another.\",\"widgetModels\":[\"t5-small\"],\"youtubeId\":\"1JvfrvZgi6c\",\"id\":\"translation\",\"label\":\"Translation\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"unconditional-image-generation\":{\"datasets\":[{\"description\":\"The
        CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with
        600 images per class.\",\"id\":\"cifar100\"},{\"description\":\"Multiple images
        of celebrities, used for facial expression translation.\",\"id\":\"CelebA\"}],\"demo\":{\"inputs\":[{\"label\":\"Seed\",\"content\":\"42\",\"type\":\"text\"},{\"label\":\"Number
        of images to generate:\",\"content\":\"4\",\"type\":\"text\"}],\"outputs\":[{\"filename\":\"unconditional-image-generation-output.jpeg\",\"type\":\"img\"}]},\"metrics\":[{\"description\":\"The
        inception score (IS) evaluates the quality of generated images. It measures
        the diversity of the generated images (the model predictions are evenly distributed
        across all possible labels) and their 'distinction' or 'sharpness' (the model
        confidently predicts a single label for each image).\",\"id\":\"Inception
        score (IS)\"},{\"description\":\"The Fr\xE9chet Inception Distance (FID) evaluates
        the quality of images created by a generative model by calculating the distance
        between feature vectors for real and generated images.\",\"id\":\"Fre\u0107het
        Inception Distance (FID)\"}],\"models\":[{\"description\":\"High-quality image
        generation model trained on the CIFAR-10 dataset. It synthesizes images of
        the ten classes presented in the dataset using diffusion probabilistic models,
        a class of latent variable models inspired by considerations from nonequilibrium
        thermodynamics.\",\"id\":\"google/ddpm-cifar10-32\"},{\"description\":\"High-quality
        image generation model trained on the 256x256 CelebA-HQ dataset. It synthesizes
        images of faces using diffusion probabilistic models, a class of latent variable
        models inspired by considerations from nonequilibrium thermodynamics.\",\"id\":\"google/ddpm-celebahq-256\"}],\"spaces\":[{\"description\":\"An
        application that can generate realistic faces.\",\"id\":\"CompVis/celeba-latent-diffusion\"}],\"summary\":\"Unconditional
        image generation is the task of generating images with no condition in any
        context (like a prompt text or another image). Once trained, the model will
        create images that resemble its training data distribution.\",\"widgetModels\":[\"\"],\"youtubeId\":\"\",\"id\":\"unconditional-image-generation\",\"label\":\"Unconditional
        Image Generation\",\"libraries\":[]},\"visual-question-answering\":{\"datasets\":[{\"description\":\"A
        widely used dataset containing questions (with answers) about images.\",\"id\":\"Graphcore/vqa\"},{\"description\":\"A
        dataset to benchmark visual reasoning based on text in images.\",\"id\":\"textvqa\"}],\"demo\":{\"inputs\":[{\"filename\":\"elephant.jpeg\",\"type\":\"img\"},{\"label\":\"Question\",\"content\":\"What
        is in this image?\",\"type\":\"text\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"elephant\",\"score\":0.97},{\"label\":\"elephants\",\"score\":0.06},{\"label\":\"animal\",\"score\":0.003}]}]},\"isPlaceholder\":false,\"metrics\":[{\"description\":\"\",\"id\":\"accuracy\"},{\"description\":\"Measures
        how much a predicted answer differs from the ground truth based on the difference
        in their semantic meaning.\",\"id\":\"wu-palmer similarity\"}],\"models\":[{\"description\":\"A
        visual question answering model trained to convert charts and plots to text.\",\"id\":\"google/deplot\"},{\"description\":\"A
        visual question answering model trained for mathematical reasoning and chart
        derendering from images.\",\"id\":\"google/matcha-base \"},{\"description\":\"A
        strong visual question answering that answers questions from book covers.\",\"id\":\"google/pix2struct-ocrvqa-large\"}],\"spaces\":[{\"description\":\"An
        application that can answer questions based on images.\",\"id\":\"nielsr/vilt-vqa\"},{\"description\":\"An
        application that can caption images and answer questions about a given image.
        \",\"id\":\"Salesforce/BLIP\"},{\"description\":\"An application that can
        caption images and answer questions about a given image. \",\"id\":\"vumichien/Img2Prompt\"}],\"summary\":\"Visual
        Question Answering is the task of answering open-ended questions based on
        an image. They output natural language responses to natural language questions.\",\"widgetModels\":[\"dandelin/vilt-b32-finetuned-vqa\"],\"youtubeId\":\"\",\"id\":\"visual-question-answering\",\"label\":\"Visual
        Question Answering\",\"libraries\":[]},\"zero-shot-classification\":{\"datasets\":[{\"description\":\"A
        widely used dataset used to benchmark multiple variants of text classification.\",\"id\":\"glue\"},{\"description\":\"The
        Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced
        collection of 433k sentence pairs annotated with textual entailment information.\",\"id\":\"MultiNLI\"},{\"description\":\"FEVER
        is a publicly available dataset for fact extraction and verification against
        textual sources.\",\"id\":\"FEVER\"}],\"demo\":{\"inputs\":[{\"label\":\"Text
        Input\",\"content\":\"Dune is the best movie ever.\",\"type\":\"text\"},{\"label\":\"Candidate
        Labels\",\"content\":\"CINEMA, ART, MUSIC\",\"type\":\"text\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"CINEMA\",\"score\":0.9},{\"label\":\"ART\",\"score\":0.1},{\"label\":\"MUSIC\",\"score\":0}]}]},\"metrics\":[],\"models\":[{\"description\":\"Powerful
        zero-shot text classification model\",\"id\":\"facebook/bart-large-mnli\"}],\"spaces\":[],\"summary\":\"Zero-shot
        text classification is a task in natural language processing where a model
        is trained on a set of labeled examples but is then able to classify new examples
        from previously unseen classes.\",\"widgetModels\":[\"facebook/bart-large-mnli\"],\"id\":\"zero-shot-classification\",\"label\":\"Zero-Shot
        Classification\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"zero-shot-image-classification\":{\"datasets\":[{\"description\":\"\",\"id\":\"\"}],\"demo\":{\"inputs\":[{\"filename\":\"image-classification-input.jpeg\",\"type\":\"img\"},{\"label\":\"Classes\",\"content\":\"cat,
        dog, bird\",\"type\":\"text\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"Cat\",\"score\":0.664},{\"label\":\"Dog\",\"score\":0.329},{\"label\":\"Bird\",\"score\":0.008}]}]},\"metrics\":[{\"description\":\"Computes
        the number of times the correct label appears in top K labels predicted\",\"id\":\"top-K
        accuracy\"}],\"models\":[{\"description\":\"Robust image classification model
        trained on publicly available image-caption data.\",\"id\":\"openai/clip-vit-base-patch16\"},{\"description\":\"Robust
        image classification model trained on publicly available image-caption data
        trained on additional high pixel data for better performance.\",\"id\":\"openai/clip-vit-large-patch14-336\"},{\"description\":\"Strong
        image classification model for biomedical domain.\",\"id\":\"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"}],\"spaces\":[{\"description\":\"An
        application that leverages zero shot image classification to find best captions
        to generate an image. \",\"id\":\"pharma/CLIP-Interrogator\"}],\"summary\":\"Zero
        shot image classification is the task of classifying previously unseen classes
        during training of a model.\",\"widgetModels\":[\"openai/clip-vit-large-patch14-336\"],\"youtubeId\":\"\",\"id\":\"zero-shot-image-classification\",\"label\":\"Zero-Shot
        Image Classification\",\"libraries\":[\"transformers.js\"]}}"
    headers:
      Access-Control-Allow-Origin:
      - https://hub-ci.huggingface.co
      Access-Control-Expose-Headers:
      - X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,ETag,Link,Accept-Ranges,Content-Range
      Connection:
      - keep-alive
      Content-Length:
      - '56498'
      Content-Type:
      - application/json; charset=utf-8
      Date:
      - Wed, 28 Jun 2023 11:46:22 GMT
      ETag:
      - W/"dcb2-9pFYIscGsHQh4SeX6alHKqUty68"
      Vary:
      - Origin
      X-Powered-By:
      - huggingface-moon
      X-Request-Id:
      - Root=1-649c1d8e-2d39bb6e35bcb1d2482c7c50
    status:
      code: 200
      message: OK
- request:
    body: null
    headers:
      user-agent:
      - unknown/None; hf_hub/0.16.0.dev0; python/3.10.6; torch/1.12.1; tensorflow/2.11.0;
        fastcore/1.5.23
    method: POST
    uri: https://api-inference.huggingface.co/pipeline/sentence-similarity/sentence-transformers/all-MiniLM-L6-v2
  response:
    body:
      string: '[0.7785726189613342,0.4587625563144684,0.2906219959259033]'
    headers:
      Access-Control-Allow-Credentials:
      - 'true'
      Connection:
      - keep-alive
      Content-Length:
      - '58'
      Content-Type:
      - application/json
      Date:
      - Wed, 28 Jun 2023 11:46:24 GMT
      Vary:
      - Origin, Access-Control-Request-Method, Access-Control-Request-Headers
      x-compute-time:
      - '0.031'
      x-compute-type:
      - cache
      x-request-id:
      - HTPTTEayayyY6Dh2328Rx
      x-sha:
      - 7dbbc90392e2f80f3d3c277d6e90027e55de9125
    status:
      code: 200
      message: OK
    url: https://api-inference.huggingface.co/pipeline/sentence-similarity/sentence-transformers/all-MiniLM-L6-v2
version: 1
